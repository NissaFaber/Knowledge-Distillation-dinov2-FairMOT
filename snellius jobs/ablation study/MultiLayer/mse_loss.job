#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=simple_layer
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=05:00:00
#SBATCH --output=slurm_output_%A.out

module purge
module load 2023
module load Anaconda3/2023.07-2



# Activate your environment
source activate mot
# Check whether the GPU is available
cd repo/Thesis/src

echo start training

srun python -u train.py mot  --arch hrnet_18 --gpus 0 --load_model '/home/nfaber/repo/models/hrnetv2_w18_imagenet_pretrained.pth' --data_cfg '../src/lib/cfg/mot17.json' --num_epochs '5' --batch_size 1 --num_workers 12 --pretrainedHrnet True --dinov2 base --alpha 0.50 --adapt_to teacher --loss_function 'MSE' --train_param on --exp_id alpha050 --multi_layer False
echo training done
echo start eval
srun python -u track.py mot  --arch 'hrnet_18' --gpus 0 --load_model '/home/nfaber/repo/Thesis/exp/mot/alpha050/model_last.py'  --val_mot17 True  --conf_thres 0.4  --pretrainedHrnet False

echo done

